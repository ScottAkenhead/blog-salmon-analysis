{
  "hash": "3375213d5bf169300f014bd51c9ba5b8",
  "result": {
    "markdown": "---\ntitle: \"Spawner Surveys: Not Year By Year.\"\nauthor: \"Scott Akenhead\"\ndate: \"2022-08-22\"\ncategories: [analysis, spawner, survey, multi-level]\nbibliography: references.bib\nimage: \"AUC_Fig_small.png\"\n---\n\n\n\n\n\n\nA simplistic examination of surveys that estimate the number of salmon spawners in a river.\n\n# Introduction\n\nThis blog describes a simplistic examination of surveys that estimate the number of salmon spawners in a river. Several surveys are conducted each year (methods irrelevant here). Two goals\n\n1.  Convince biologists that analyzing all of the data at once is better than one year at a time. So we will be looking for an overall pattern of salmon, a shape, and extracting a *years effect* on that pattern -- which will include yearly abundance of salmon, the goal of the surveys.\n\n2.  Compare the results of conventional year-by-year approach to the result from years-together.\n\nThe analysis is deliberately simple, with more sophisticated approaches planned for subsequent blogs.\n\n## Area Under the Curve (AUC)\n\nConvention determines salmon spawner abundance by linear interpolation between survey estimates each year separately, with assumptions about timing for zero abundance before and after the surveys: a **trapezoid** [@parken2003]. There is an alternative, where all years are combined to\n\n(a) determine the general temporal pattern of spawner abundance (\"*salmon days*), perhaps Gaussian if early arrivals and late survivors viewed as simply rare, perhaps Beta if ordained zero.\n(b) given (a), estimate the change to that shape for each year, perhaps skewed to be earlier or later than the pattern across all years,\n(c) given (a) and (b), determine the total abundance for each year, and\n(d) from (a), (b), and (c), determine the observation error for individual surveys, $\\sigma_{obs},$ perhaps as a fixed percentage error ($\\sigma_{obs}=20\\%$ of true abundance, implying a log transform) rather than an absolute error ($\\sigma_{obs} = 20$ spawners).\n\n# Example\n\n## Test Dataset\n\nAs test data, the abundance estimates for 21 surveys over 4 years from Parken *et al.* 2003 (@fig-Parken). The numbers are reproduced only approximately (@tbl-raw), and plotted in @fig-raw with the observations from all years combined.\n\n![Plot of trapezoids from salmon spawner surveys from Parken et al (2003).](AUC%20Fig%202%20Parken%20Bailey%20Irvine%202003.png){#fig-Parken}\n\n\n::: {#tbl-raw .cell tbl-cap='Salmon counts as read from Parken _et al._ (2003).'}\n\n```{.r .cell-code}\ndat <- data.frame(\n    year=c(rep(1996,5),rep(1997,5),rep(1998,6),rep(1999,5)), \n    day=c(5,9,13,18,23,  5,9,13,15,19,  5,9,13,15,19,23,  5,9,13,15,19),\n    count=c(850,6000,11000,5000,1000,  500,2700,4000,4100,2500,\n            15,40,200,500,700,600,  500,2500,4100,4000,1300) )\nkbl(dat)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> year </th>\n   <th style=\"text-align:right;\"> day </th>\n   <th style=\"text-align:right;\"> count </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1996 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 850 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1996 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 6000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1996 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 11000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1996 </td>\n   <td style=\"text-align:right;\"> 18 </td>\n   <td style=\"text-align:right;\"> 5000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1996 </td>\n   <td style=\"text-align:right;\"> 23 </td>\n   <td style=\"text-align:right;\"> 1000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1997 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 500 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1997 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 2700 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1997 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 4000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1997 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 4100 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1997 </td>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:right;\"> 2500 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1998 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1998 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1998 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 200 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1998 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 500 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1998 </td>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:right;\"> 700 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1998 </td>\n   <td style=\"text-align:right;\"> 23 </td>\n   <td style=\"text-align:right;\"> 600 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1999 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 500 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1999 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 2500 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1999 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 4100 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1999 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 4000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1999 </td>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:right;\"> 1300 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nSetPar()\nplot(dat$day, 0.001*dat$count, xlim=c(0,36),ylim=c(0,12),yaxs=\"i\",\n     xlab='Day from 31 October', ylab=\"Salmon Count ('000)\")\nAxis34()\n```\n\n::: {.cell-output-display}\n![Salmon counts as read from Parken _et al._ (2003) by day with all years combined.](index_files/figure-html/fig-raw-1.png){#fig-raw width=576}\n:::\n:::\n\n\n### Scale by Maximum Count\n\nFirst guess at shape is to scale observations by maximum each year, then fit a loess curve (@fig-scaled). It is necessary to sort the data by day for loess. Zeros were added outside the dates of observations, as per using a trapezoid.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxa <- by(dat$count,dat$year, max) # get maximums by year\nxb <- by(dat$count,dat$year, length) # number of observations by year\n# one value per year, copy for each obs in a year.\nxc <- NULL  # temporary\nfor(j in 1:4) xc <- c(xc,rep( xa[[j]], xb[[j]]) )    \ndat$max <- xc \ndat$scaled <- dat$count / dat$max\nxa <- dat[order(dat$day), c(\"day\",\"scaled\")] # extract and sort\nlo <- loess(scaled ~ day, data=xa) # fit smoother to sorted data\n#  range for resulting smooth line is range of observed days\ndays = 5:23\nloPred <- predict(lo, data.frame(day = days) ) # all days in range\nloPred <-data.frame(pred=loPred,day=days)      # not just observed day\n\nSetPar()\nplot(scaled~day,data=xa, xlim=c(0,36),\n     xlab='Day from 31 October', ylab='Scaled Count')\nlines(loPred$pred  ~ loPred$day ) \nAxis34()\n```\n\n::: {.cell-output-display}\n![Salmon counts scaled (from 0 to 1) by the maximum count each year.](index_files/figure-html/fig-scaled-1.png){#fig-scaled width=576}\n:::\n:::\n\n\n## Shift Observations by Day of Maximum\n\nOne of the years has a large change in timing, adding scatter to the preceding plot. We can reduce that scatter shifting (aligning) observations according to date of maximum abundance within years (@fig-shifted). This is inaccurate (a hueristic) but demonstrates a parameter (timing) that will be estimated better subsequently. The variable *shift* is the shifted value for day of observation, with negative values and the peak will be at day *shift*. Apply the shift and plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeak <-dat$day[ which(dat$count == dat$max)] # peak day by year \nxa <- NULL # temporary\nfor(j in 1:4) xa <- c(xa,rep( peak[j], xb[[j]]) )    \ndat$shift <- dat$day - xa \nxa <- dat[order(dat$shift), c(\"shift\",\"scaled\")] # extract and sort\n# add zeros outside range of obs.\nxa0 <- rbind (c(-15,0), xa, c(15,0))\nlo <- loess(scaled ~ shift, data=xa0) # smoother #,span=0.5)\nloPred <- predict(lo, data.frame(shift= -15:15) )\nloPred <-data.frame(pred=loPred,shift=c(-15:15))\n# redo preceding plot\nSetPar()\nplot(scaled~shift,data=xa0, xlim=c(-18,18), ylim=c(0,1), yaxs=\"i\",\n     xlab='Day from Year Peak', ylab='Scaled Count')\nlines(pred ~shift, data = loPred ) # \nAxis34()\n```\n\n::: {.cell-output-display}\n![Salmon counts shifted and scaled (from 0 to 1) by the maximum count, and the corresponding day, within each year.](index_files/figure-html/fig-shifted-1.png){#fig-shifted width=576}\n:::\n:::\n\n\n## Assume Gaussian\n\n@fig-shifted suggests a symmetric Gaussian (\"normal\") probability density distribution (PDD):\n\n\n$$ \\textbf{G}(x) = (2\\pi \\sigma)^{-1/2} ~ e^{ -(x-\\mu)^2 /2\\sigma ^2} $$\n\n\nwith $\\mu \\approx 0 \\text{ and } \\sigma \\approx 5$ days as noticed from crude scaling of abundance and shifting of time for each year.\n\nAs well as the parameters $\\mu$  and $\\sigma,$ additional parameter $\\eta$ is required for total abundance, the integral of the PDD and proportional to maximum abundance.\n\nThe integral of the Gauss-Laplace function $$\\int{e^{-x^2}} = (2\\pi)^{1/2} \\approx 2.5$$ For a PDD, the integral must be 1, and this is effected by the term $(2\\pi \\sigma )^{-1/2} \\approx 0.4.$ This is the maximum of the Gaussian PDD at $\\mu = 0$ when $\\sigma = 1.$ If the abundance $\\eta= 1/0.3989422804 = 2.505528$ then the maximum for the curve is 1 and the integral $\\approx 2.5.$ Similarly, when $\\sigma = 2$ the maximum is 1 when $\\eta= 2/0.4 = 5$ and for $\\sigma = 5, ~ \\eta=12.5$\n\nIn the scaled and shifted data the maximum was assigned to be 1 and it can be observed that $\\sigma \\approx 5.$ Fitting a Gaussian distribution with 3 parameters $(\\eta,\\mu,\\sigma)$ to all years combined, and after the rough scaling and shifting, is accomplished by the following chunk.\n\nThis chunk is a non-linear fit, obtained by searching for the parameters that have the lowest SSQ, the lowest sum of squared differences between observed and predicted (fitted) values. A function *Gssq()* is defined for this; it uses the built-in function stats::dnorm (normal probability density) to predict $\\hat{y}$ for each observed day, given trial estimates for $\\mu,~ \\sigma,~ \\eta$ and determines the SSQ. Then *Gssq()* is a parameter for the built-in function stats::optim that searches for value of the parameters that minimize SSQ. The other parameters for *optim()* are (1) a starting guess at parameters, and (2) the observed data: 21 values of *day* and *abundance*.\n\nFitting is by searching for the set of parameters that minimizes the sum of squared differnces (SSQ) between predicted $\\hat{y}$ and observed $y$, $$ SSQ = \\sum{(y - \\hat{y})^2}$$\n\nGiven the SSQ at best fit, we can determine how well the data is fitted by a Gaussian, $\\hat{y},$ compared to fitting the mean abundance over all days, $\\bar{y},$ a default model, by determining the reduction in SSQ: $$ r^2 = 1- \\frac{\\sum (y-\\hat{y})^2}{\\sum (y-\\bar{y})^2 }.$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGssq <- function(par, xy){\n    x <- xy[,1]; y <-  xy[,2];\n    eta <- par[1]; mu <- par[2]; sigma <- par[3];\n    yhat <- eta * dnorm(x,mu,sigma) # make the prediction\n    ssq <- sum( (y-yhat)^2)   # to be minimized\n    return(ssq)\n}\n# starting guess for the search: eta=12.5, mu=0, sigma=5\nGfit <- optim(par=c(12.5, 0, 5), Gssq, xy=xa )\n# pull out the fitted parameters\npar = Gfit$par; pr = round(par,2)\n# Gfit$value is SSQ: fitted to observed. Get SSQ: mean to obs.\nSSQ <- sum( (xa[,2] - mean(xa[,2]))^2 ) #  y minus mean(y)\nr2 <- round (1- Gfit$value/SSQ, 2) # percent reduction in SSQ\ncat('r^2 = ', r2, ', eta = ',pr[1], ', mu = ',pr[2],\n    ', sigma = ', pr[3], '\\n', sep='')\n->  r^2 = 0.94, eta = 11.02, mu = 0.05, sigma = 4.29\n```\n:::\n\n\nThe result provides $\\sigma \\approx 4.3$ which is a more narrow spread than noticing the inflection point in the Gaussian, at $(1 ~\\sigma),$ was about $\\pm5$ from the mean. The fitted maximum will necessarily be close to 1 because of our scaling, but in this case greater than 1, given $\\sigma / 0.4 = 10.72$ but $\\eta = 11.2,$ due to our rough scaling.\n\nThe fitted line is compared visually to the data by the following chunk. A smooth Gaussian curve is plotted by estimating a value for 18 days before and after the mean day.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(-18:18) # 37 points for a smooth(er) line.\npred <- par[1] * dnorm(x, par[2],par[3]) # eta, mu, sigma\nSetPar()\nplot(scaled~shift,data=xa, xlim=c(-18,18), ylim=c(0,1.2),yaxs=\"i\",\n     xlab='Day from Annual Peak', ylab='Scaled Count')\nlines(pred ~ x)\nAxis34()\n```\n\n::: {.cell-output-display}\n![Gaussian probability density fitted to the scaled and shifted counts.](index_files/figure-html/fig-norm-1.png){#fig-norm width=576}\n:::\n:::\n\n\n## Year Effects\n\nThe preceding identified the overall pattern as Gaussian with $\\sigma \\approx 4.3.$ If $\\sigma$ is assumed to be known and constant across years, as @fig-norm suggests, then only two observations per year are required to estimate total abundance: $\\eta_t,~ \\mu_t.$ To clarify: a single observation of early and small abundance cannot distinguish a small run with average timing from a big run that is late; one subsequent observation can make that distinction (in theory, if observations precise and salmon behave).\n\nThis test dataset has 5 or 6 observations per year, so precision is improved and over-fitting is reduced compared to (a) fitting 3 parameters to each year separately, or (b) interpolating to an aribitrary trapezoid that requires arbitrarily placed zeros.\n\nThe next step estimates 2 parameters for year effects: $\\eta_{year}$ (abundance) and $\\mu_{year}$ (timing) for each of 4 years, and 1 estimate for $\\sigma$ (spread, constant) from 21 observations. This is a ratio of $2.\\bar{3}$ data points to parameters, and could be improved by adding more years: for 20 years \\@ 5 surveys/year the ratio would be $11.\\bar{1}.$\n\n\n$$\\hat{y}_{day, year} = \\eta_{year} ~ \\textbf{G}(x_{year},\\mu_{year},\\sigma)$$\n\n::: {.cell}\n\n```{.r .cell-code}\n# GssqAll <- function(par, dat){\n#     # dat columns: year, day, count\n#     # par is 9 parameters\n#     eta <- par[1:4]; # abundance each year, length 4\n#     mu <- par[5:8]; # timing  each year, length 4\n#     sigma <- par[9]; # spread every yearr, length 1\n#     years = unique(dat[,1]) # 1996 to 1999\n#     ybar <- numeric(length(dat[,1])) # the predicted, length 21\n#     m = 0  # starting index for output vector ybar\n#     for (j in 1:length(years)) {  # 4 years: 1996 to 1999\n#         k <- which(dat[,1] == years[j]) # find rows in dat for each year\n#         for (i in k){ # each observed day in that year\n#             m <- m+1 # advance index for output\n#             ybar[m]  <- eta[j] * dnorm(dat[m,2],mu[j],sigma)\n#             # predicted: abundance times normal(day, timing, spread)\n#         }\n#     }\n#     ssq <- sum( (dat[,3]-ybar)^2) # result of trial values for par\n#     return(ssq)\n# }\nGssqAll <- function(par, dat){\n    # dat columns: year, day, count\n    # par is 9 parameters\n    eta <- par[1:4]; # abundance each year, length 4\n    mu <- par[5:8]; # timing  each year, length 4\n    sigma <- par[9]; # spread every yearr, length 1\n    years = unique(dat[,1]) # 1996 to 1999\n    ybar <- numeric(length(dat[,1])) # the predicted, length 21\n    for (j in 1:length(years)) {  # 4 years: 1996 to 1999\n        k <- which(dat[,1] == years[j]) # find rows in dat for each year\n        ybar[k]  <- eta[j] * dnorm(dat[k,2],mu[j],sigma) # all of k\n        # predicted: abundance times normal(day, timing, spread)\n    }\n    ssq <- sum( (dat[,3]-ybar)^2) # result of trial values for par\n    return(ssq)\n}\n\n# par is eta(4),  mu (4), sigma (1)\npar <-  numeric(9)\n# starting guess for search is eta = 2*sigma*max.\n# where max is max observed count in each year.\npar[1:4] <- c(2*4.3*11000, 2*4.3*4100, 2*4.3*700, 2*4.3*4100)\n#  result: 94600 35260  6020 35260\n#  start for yearly timing (mu) is day of observed maximum each year\npar[5:8] <- c(15,15,19,15)\n# start for spread, sigma, is from preceding fit.\npar[9] <- 4.3\n# the required precision of fit, reltol, is reduced from the default.\ncontrol <- list(reltol=0.01)\nGfit <- optim(par, GssqAll, dat=dat[,1:3], control )\npar = Gfit$par; # 9 fitted parameters\npr[1:4] = round(par[1:4], 0) # rounded for printing.\npr[5:9] = round(par[5:9], 2) \na <- data.frame (Year=1996:1999,eta=pr[1:4],mu=pr[5:8], sigma=pr[9])\nkbl(a)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Year </th>\n   <th style=\"text-align:right;\"> eta </th>\n   <th style=\"text-align:right;\"> mu </th>\n   <th style=\"text-align:right;\"> sigma </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1996 </td>\n   <td style=\"text-align:right;\"> 99334 </td>\n   <td style=\"text-align:right;\"> 13.22 </td>\n   <td style=\"text-align:right;\"> 3.72 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1997 </td>\n   <td style=\"text-align:right;\"> 39584 </td>\n   <td style=\"text-align:right;\"> 13.82 </td>\n   <td style=\"text-align:right;\"> 3.72 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1998 </td>\n   <td style=\"text-align:right;\"> 6024 </td>\n   <td style=\"text-align:right;\"> 19.32 </td>\n   <td style=\"text-align:right;\"> 3.72 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1999 </td>\n   <td style=\"text-align:right;\"> 35662 </td>\n   <td style=\"text-align:right;\"> 13.20 </td>\n   <td style=\"text-align:right;\"> 3.72 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# r^2 for overall fit, all 4 years at once.\nSSQ <- sum( (dat[,3] - mean(xa[,2]))^2 ) # SSQ from mean\nr2  <-  1- Gfit$value / SSQ\nr2  <- 100*round (r2,2) # as percent\n# print( paste('r^2 = ', r2,'%', sep='') ) # save for text\n```\n:::\n\n\nThe resulting fit, with $r^2 =$ 99%, minimizes SSQ over all years, despite large differences in abundance within and between years. Perhaps large abundances have a larger effect than small on the fit determined by SSQ. To investigate, determine the fit for each year as separate $r^2$ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\neta <- par[1:4];  mu <- par[5:8]; sigma <- par[9]; \nyear <- unique(dat[,1]) # 1996 to 1999\nnyear <- length(year)   #4 \nyhat <- NULL # will be 21 predicted \nr2 <- numeric(nyear)\nfor (j in 1:nyear) {\n    k <- which(dat[,1] == year[j]) # index for rows for this year\n    yhat[k] <- eta[j] * dnorm(dat[k,2], mu[j], sigma) # a vector\n    # dat[,2] is day. dat[,3] is count\n    ssq_fit <- sum( (dat[k,3] - yhat[k])^2)\n    ssq_raw <- sum( (dat[k,3] - mean(dat[k,3]) )^2)\n    r2[j] <- 1 - ssq_fit / ssq_raw\n}\na <- data.frame(Year=year, Abundance=round(eta,0), r2=round(r2,2) ) \nkbl(a) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Year </th>\n   <th style=\"text-align:right;\"> Abundance </th>\n   <th style=\"text-align:right;\"> r2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1996 </td>\n   <td style=\"text-align:right;\"> 99334 </td>\n   <td style=\"text-align:right;\"> 0.99 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1997 </td>\n   <td style=\"text-align:right;\"> 39584 </td>\n   <td style=\"text-align:right;\"> 0.81 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1998 </td>\n   <td style=\"text-align:right;\"> 6024 </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1999 </td>\n   <td style=\"text-align:right;\"> 35662 </td>\n   <td style=\"text-align:right;\"> 0.93 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nCompare the estimates of \"fish days\" from trapezoidal AUC and from fitting Gaussian distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na$AUC <- c(101211, 56010, 9847,45263)\na$PercentDiff <- round( 100*(a$AUC-a$Abundance)/a$Abundance, 2)\nkbl(a)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Year </th>\n   <th style=\"text-align:right;\"> Abundance </th>\n   <th style=\"text-align:right;\"> r2 </th>\n   <th style=\"text-align:right;\"> AUC </th>\n   <th style=\"text-align:right;\"> PercentDiff </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1996 </td>\n   <td style=\"text-align:right;\"> 99334 </td>\n   <td style=\"text-align:right;\"> 0.99 </td>\n   <td style=\"text-align:right;\"> 101211 </td>\n   <td style=\"text-align:right;\"> 1.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1997 </td>\n   <td style=\"text-align:right;\"> 39584 </td>\n   <td style=\"text-align:right;\"> 0.81 </td>\n   <td style=\"text-align:right;\"> 56010 </td>\n   <td style=\"text-align:right;\"> 41.50 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1998 </td>\n   <td style=\"text-align:right;\"> 6024 </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n   <td style=\"text-align:right;\"> 9847 </td>\n   <td style=\"text-align:right;\"> 63.46 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1999 </td>\n   <td style=\"text-align:right;\"> 35662 </td>\n   <td style=\"text-align:right;\"> 0.93 </td>\n   <td style=\"text-align:right;\"> 45263 </td>\n   <td style=\"text-align:right;\"> 26.92 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\na1 <- round(mean(abs(a$PercentDiff)),0)\n#print( paste('The mean difference is ', a1, '%.',sep='' )) \n```\n:::\n\n\nThe mean difference in annual estimates of *fish days* is 33%.'\n\nThe four Gaussian distributions, and the observed and predicted counts, are plotted in @fig-years by the following chunk.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# GssqAllPred <- function(par, dat){\n#     # same as GssqAll but returns the predictions. AND smooth curves.\n#     eta <- par[1:4];   mu <- par[5:8]; sigma <- par[9];\n#     years <-  unique(dat[,1]) # 1996 to 1999\n#     nyears <- length(years)\n#     x <- 1:30 # days for smooth\n#     ybar <- numeric(dim(dat)[1] ) # predicted for each day\n#     smooth <- NULL # predicted as smooth curve\n#     m = 0 # row of output, total 21\n#     for (j in 1:nyears) {  # 1 to 4, 1996 to 1999\n#         k <- which(dat[,1] == years[j]) # find rows in a for each year\n#         for (i in k){ # each row in that year\n#             m <- m+1 # row of output\n#             ybar[m]  <- eta[j] * dnorm(dat[m,2],mu[j],sigma)\n#         }\n#         smooth <- c(smooth, eta[j] * dnorm(x,mu[j],sigma)) \n#     }\n#     a <- list(ybar=ybar, smooth=smooth)\n#     return(a)\n# }\n\nx <- c(1:30) # days of September\n# yhat is predicted counts for the days observed\nymax= 1.1e-3 * max(yhat)  # plot as thousands\nSetPar()\nplot(1e-3*yhat ~ dat[,2], xlim=c(1,30), ylim=c(0, ymax), yaxs=\"i\",\n     xlab='Day', ylab=\"Salmon Counts ('000)\" )\npoints(1e-3*dat[,3] ~ dat[,2], pch=1) # observed\nfor (j in 1:4){\n    y  <- eta[j] * dnorm(x, mu[j], sigma) # smooth curve\n    lines(1e-3*y ~ x)\n}\nAxis34()\n```\n\n::: {.cell-output-display}\n![Four Gaussian curves with the same spread describe the observed salmon abundance. 5 or 6 observations in each year estimate just 2 parameters: abundance and timing.](index_files/figure-html/fig-years-1.png){#fig-years width=576}\n:::\n:::\n\n\n# Discussion\n\nThe overall result of fitting Gaussians to four years of data simultaneously produced $r^2=99\\%$ which seems excellent, but only superficially. At least we have\n\n1.  estimates of how well the data are fitted by the model;\n\n2.  a better way to interpolate between observations, the continuous Gaussian; and\n\n3.  removed the need for, and effect from, arbitrary estimates for timing of zero abundance.\n\nBut there is a tendency for\n\n1.  larger abundances to have better fit ($r^2$ by year), suggesting that large observations have a bigger effect than small; and\n\n2.  years with the largest difference between Gaussian and AUC have lower fit.\n\nThe range of observations, from 15 to 11,000, suggests that the sampling error, the precision of observations, is proportional to the observation, as in $\\pm 20 \\%,$ as opposed to a fixed error, as in $\\sigma=20$ *fish.* Ordinary linear regression assumes fixed error, and the non-linear fitting mimics that. Proportional sampling error, where the error is essentially a multiplier, implies the fit should be to log abundances, so that log(error) is additive.\n\n## Next Step\n\nSaved for a subsequent post: Determine how to fit the log of the Gaussian to the log of the counts. I discovered a mathematical **trick** that makes this simple.\n\n## Down the Road\n\nThese regressions should be more rigorous, as Bayesian multi-level regression. And what about multiple rivers in the same area that are surveyed in the same years -- can the model can be extended to river effects similar to year effects?\n\n# References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}