{"title":"Spawner Surveys: Not Year By Year.","markdown":{"yaml":{"title":"Spawner Surveys: Not Year By Year.","author":"Scott Akenhead","date":"2022-08-22","categories":["analysis","spawner","survey","multi-level"],"bibliography":"references.bib"},"headingText":"| include: false","containsRefs":false,"markdown":"\n\nMy first post via [Quarto blog](https://quarto.org/docs/websites/website-blog.html \"link:\"), please forebear.\n\n![](thumbnail.jpg)\n\nSince this post doesn't specify an explicit `image`, the first image in the post will be used in the listing page of posts.\n\n```{r, setup}\n#| echo: false\nlibrary(knitr); library(kableExtra);library(kableExtra);\nknitr::opts_chunk$set(\n  comment = '', fig.width = 6, fig.height = 6, collapse = TRUE, hold = TRUE)\n```\n\n```{r, local}\n#| include: false\n#| echo: false\n# ________ local functions _________\nAxis34 <- function(){\n\taxis(side=3,labels=FALSE);axis(side=4,labels=FALSE);box(lwd=1.5)\n} \nSetPar <-function(){\n    par(tcl=0.2,          # axis tics small inside vs -0.5: large outside\n   \tlas=1,                # axis numbers always horizontal\n   \tcex.axis=1.2,         # larger axis numbers \n\tcex.lab=1.5,          # big axis labels\n\tmgp=c(1.75, 0.2, 0),  # closer axis labels and numbers\n   \tpch=20\t              # small dots\n)}\n```\n\n# Introduction\n\nThis blog describes a simplistic examination of surveys that estimate the number of salmon spawners in a river. Several surveys are conducted each year (methods irrelevant here). Two goals\n\n1.  Convince biologists that analyzing all of the data at once is better than one year at a time. So we will be looking for an overall pattern of salmon, a shape, and extracting a *years effect* on that pattern -- which will include yearly abundance of salmon, the goal of the surveys.\n\n2.  Compare the results of conventional year-by-year approach to the result from years-together.\n\nThe analysis is deliberately simple, with more sophisticated approaches planned for subsequent blogs.\n\n## Area Under the Curve (AUC)\n\nConvention determines salmon spawner abundance by linear interpolation between survey estimates each year separately, with assumptions about timing for zero abundance before and after the surveys: a **trapezoid** [@parken2003]. There is an alternative, where all years are combined to\n\n(a) determine the general temporal pattern of spawner abundance (\"*salmon days*), perhaps Gaussian if early arrivals and late survivors viewed as simply rare, perhaps Beta if ordained zero.\n(b) given (a), estimate the change to that shape for each year, perhaps skewed to be earlier or later than the pattern across all years,\n(c) given (a) and (b), determine the total abundance for each year, and\n(d) from (a), (b), and (c), determine the observation error for individual surveys, $\\sigma_{obs},$ perhaps as a fixed percentage error ($\\sigma_{obs}=20\\%$ of true abundance, implying a log transform) rather than an absolute error ($\\sigma_{obs} = 20$ spawners).\n\n# Example\n\n## Test Dataset\n\nAs test data, the abundance estimates for 21 surveys over 4 years from Parken *et al.* 2003 (@fig-Parken). The numbers are reproduced only approximately (@tbl-raw), and plotted in @fig-raw with the observations from all years combined.\n\n![Plot of trapezoids from salmon spawner surveys from Parken et al (2003).](AUC%20Fig%202%20Parken%20Bailey%20Irvine%202003.png){#fig-Parken}\n\n```{r}\n#| label: tbl-raw\n#| tbl-cap: \"Salmon counts as read from Parken _et al._ (2003).\"\n\ndat <- data.frame(\n    year=c(rep(1996,5),rep(1997,5),rep(1998,6),rep(1999,5)), \n    day=c(5,9,13,18,23,  5,9,13,15,19,  5,9,13,15,19,23,  5,9,13,15,19),\n    count=c(850,6000,11000,5000,1000,  500,2700,4000,4100,2500,\n            15,40,200,500,700,600,  500,2500,4100,4000,1300) )\nkbl(dat)\n```\n\n```{r}\n#| label: fig-raw\n#| fig-cap: \"Salmon counts as read from Parken _et al._ (2003) by day with all years combined.\"\n\nSetPar()\nplot(dat$day, 0.001*dat$count, xlim=c(0,36),ylim=c(0,12),yaxs=\"i\",\n     xlab='Day from 31 October', ylab=\"Salmon Count ('000)\")\nAxis34()\n```\n\n### Scale by Maximum Count\n\nFirst guess at shape is to scale observations by maximum each year, then fit a loess curve (@fig-scaled). It is necessary to sort the data by day for loess. Zeros were added outside the dates of observations, as per using a trapezoid.\n\n```{r}\n#| label: fig-scaled\n#| fig-cap: \"Salmon counts scaled (from 0 to 1) by the maximum count each year.\" \n\nxa <- by(dat$count,dat$year, max) # get maximums by year\nxb <- by(dat$count,dat$year, length) # number of observations by year\n# one value per year, copy for each obs in a year.\nxc <- NULL  # temporary\nfor(j in 1:4) xc <- c(xc,rep( xa[[j]], xb[[j]]) )    \ndat$max <- xc \ndat$scaled <- dat$count / dat$max\nxa <- dat[order(dat$day), c(\"day\",\"scaled\")] # extract and sort\nlo <- loess(scaled ~ day, data=xa) # fit smoother to sorted data\n#  range for resulting smooth line is range of observed days\ndays = 5:23\nloPred <- predict(lo, data.frame(day = days) ) # all days in range\nloPred <-data.frame(pred=loPred,day=days)      # not just observed day\n\nSetPar()\nplot(scaled~day,data=xa, xlim=c(0,36),\n     xlab='Day from 31 October', ylab='Scaled Count')\nlines(loPred$pred  ~ loPred$day ) \nAxis34()\n```\n\n## Shift Observations by Day of Maximum\n\nOne of the years has a large change in timing, adding scatter to the preceding plot. We can reduce that scatter shifting (aligning) observations according to date of maximum abundance within years (@fig-shifted). This is inaccurate (a hueristic) but demonstrates a parameter (timing) that will be estimated better subsequently. The variable *shift* is the shifted value for day of observation, with negative values and the peak will be at day *shift*. Apply the shift and plot.\n\n```{r}\n#| label: fig-shifted\n#| fig-cap: \"Salmon counts shifted and scaled (from 0 to 1) by the maximum count, and the corresponding day, within each year.\" \n\npeak <-dat$day[ which(dat$count == dat$max)] # peak day by year \nxa <- NULL # temporary\nfor(j in 1:4) xa <- c(xa,rep( peak[j], xb[[j]]) )    \ndat$shift <- dat$day - xa \nxa <- dat[order(dat$shift), c(\"shift\",\"scaled\")] # extract and sort\n# add zeros outside range of obs.\nxa0 <- rbind (c(-15,0), xa, c(15,0))\nlo <- loess(scaled ~ shift, data=xa0) # smoother #,span=0.5)\nloPred <- predict(lo, data.frame(shift= -15:15) )\nloPred <-data.frame(pred=loPred,shift=c(-15:15))\n# redo preceding plot\nSetPar()\nplot(scaled~shift,data=xa0, xlim=c(-18,18), ylim=c(0,1), yaxs=\"i\",\n     xlab='Day from Year Peak', ylab='Scaled Count')\nlines(pred ~shift, data = loPred ) # \nAxis34()\n```\n\n## Assume Gaussian\n\n@fig-shifted suggests a symmetric Gaussian (\"normal\") probability density distribution (PDD):\n\n$$ \\textbf{G}(x) = (2\\pi \\sigma)^{-1/2} ~ e^{ -(x-\\mu)^2 /2\\sigma ^2} $$\n\nwith $\\mu \\approx 0 \\text{ and } \\sigma \\approx 5$ days as noticed from crude scaling of abundance and shifting of time for each year.\n\nAs well as the parameters $\\mu$  and $\\sigma,$ additional parameter $\\eta$ is required for total abundance, the integral of the PDD and proportional to maximum abundance.\n\nThe integral of the Gauss-Laplace function $$\\int{e^{-x^2}} = (2\\pi)^{1/2} \\approx 2.5$$ For a PDD, the integral must be 1, and this is effected by the term $(2\\pi \\sigma )^{-1/2} \\approx 0.4.$ This is the maximum of the Gaussian PDD at $\\mu = 0$ when $\\sigma = 1.$ If the abundance $\\eta= 1/0.3989422804 = 2.505528$ then the maximum for the curve is 1 and the integral $\\approx 2.5.$ Similarly, when $\\sigma = 2$ the maximum is 1 when $\\eta= 2/0.4 = 5$ and for $\\sigma = 5, ~ \\eta=12.5$\n\nIn the scaled and shifted data the maximum was assigned to be 1 and it was observed that $\\sigma \\approx 5.$ Fitting a Gaussian distribution with 3 parameters $(\\eta,\\mu,\\sigma)$ to all years combined, and after the rough scaling and shifting, is accomplished by the following chunk.\n\nThis chunk is a non-linear fit, obtained by searching for the parameters that have the lowest SSQ, the lowest sum of squared differences between observed and predicted (fitted) values. A function *Gssq()* is defined for this; it uses the built-in function stats::dnorm (normal probability density) to predict $\\hat{y}$ for each observed day, given trial estimates for $\\mu,~ \\sigma,~ \\eta$ and determines the SSQ. Then *Gssq()* is a parameter for the built-in function stats::optim that searches for value of the parameters that minimize SSQ. The other parameters for *optim()* are (1) a starting guess at parameters, and (2) the observed data: 21 values of *day* and *abundance*.\n\nFitting is by searching for the set of parameters that minimizes the sum of squared differnces (SSQ) between predicted $\\hat{y}$ and observed $y$, $$ SSQ = \\sum{(y - \\hat{y})^2}$$\n\nGiven the SSQ at best fit, we can determine how well the data is fitted by a Gaussian, $\\hat{y},$ compared to fitting the mean abundance over all days, $\\bar{y},$ a default model, by determining the reduction in SSQ: $$ r^2 = 1- \\frac{\\sum (y-\\hat{y})^2}{\\sum (y-\\bar{y})^2 }.$$\n\n```{r}\nGssq <- function(par, xy){\n    x <- xy[,1]; y<-  xy[,2];\n    eta <- par[1]; mu <- par[2]; sigma <- par[3];\n    ybar <- eta * dnorm(x,mu,sigma)\n    ssq <- sum( (y-ybar)^2)\n    return(ssq)\n}\nGfit <- optim(par=c(12.5, 0, 5), Gssq, xy=xa )\npar = Gfit$par; pr = round(par,2)\nSSQ <- sum( (xa[,2] - mean(xa[,2]))^2 ) # sum of y minus y-hat, squared\nr2 <- round (1- Gfit$value/SSQ, 2) # Gfit$value is SSQ fitted. \ncat('r^2 = ', r2, ', eta = ',pr[1], ', mu = ',pr[2],\n    ', sigma = ', pr[3], '\\n', sep='')\n```\n\nThe result provides $\\sigma \\approx 4.3$ which is a more narrow spread than noticing the inflection point in the Gaussian, at $(1 ~\\sigma),$ was about $\\pm5$ from the mean. The fitted maximum will necessarily be close to 1 because of our scaling, but in this case greater than 1, given $\\sigma / 0.4 = 10.72$ but $\\eta = 11.2,$ due to our rough scaling.\n\nThe fitted line is compared visually to the data by the following chunk. A smooth Gaussian curve is plotted by estimating a value for 18 days before and after the mean day.\n\n```{r}\n#| label: fig-norm\n#| fig-cap: \"Gaussian probability density fitted to the scaled and shifted counts.\"\n\nx <- c(-18:18) # 37 points for a smooth(er) line.\npred <- par[1] * dnorm(x, par[2],par[3]) # eta, mu, sigma\nSetPar()\nplot(scaled~shift,data=xa, xlim=c(-18,18), ylim=c(0,1.2),yaxs=\"i\",\n     xlab='Day from Annual Peak', ylab='Scaled Count')\nlines(pred ~ x)\nAxis34()\n```\n\n## Year Effects\n\nThe preceding identified the overall pattern as Gaussian with $\\sigma \\approx 4.3.$ If $\\sigma$ is assumed to be constant across years, as @fig-norm suggests, then only two observations per year are required to estimate total abundance: $\\eta_t,~ \\mu_t.$ To clarify: a single observation of early and small abundance cannot distinguish a small run with average timing from a big run that is late; one subsequent observation can (in theory).\n\nThis test dataset has 5 or 6 observations per year, so precision is improved and over-fitting is reduced compared to (a) fitting 3 parameters to each year separately, or (b) interpolating a trapezoid that includes arbitrarily placed zeros.\n\nThe next step estimates 8 parameters for year effects: $\\eta$ (abundance) and $\\mu$ (timing) for each of 4 years, and 1 estimate for $\\sigma$ (spread) from 21 observations. The ratio of $2.\\bar{3}$ data points to parameters might be improved by adding more years: 20 years \\@ 5 surveys/year would be $11.\\bar{1}.$\n\n$$\\hat{y}_{day, year} = \\eta_{year} ~ \\textbf{G}(x_{year},\\mu_{year},\\sigma)$$\n\n```{r}\n#| warning: false\n\nGssqAll <- function(par, dat){\n    # dat columns: year, day, count\n    # par is 9 parameters\n    eta <- par[1:4]; # abundance each year, length 4\n    mu <- par[5:8]; # timing  each year, length 4\n    sigma <- par[9]; # spread every yearr, length 1\n    years = unique(dat[,1]) # 1996 to 1999\n    ybar <- numeric(length(dat[,1])) # the predicted, length 21\n    m = 0  # starting index for output vector ybar\n    for (j in 1:length(years)) {  # 4 years: 1996 to 1999\n        k <- which(dat[,1] == years[j]) # find rows in dat for each year\n        for (i in k){ # each observed day in that year\n            m <- m+1 # advance index for output\n            ybar[m]  <- eta[j] * dnorm(dat[m,2],mu[j],sigma)\n            # predicted: abundance times normal(day, timing, spread)\n        }\n    }\n    ssq <- sum( (dat[,3]-ybar)^2) # result of trial values for par\n    return(ssq)\n}\n# par is total abundance 4 years:eta; timing 4 years: mu, and \n# spread for all years: sigma. total 9 parameters.\npar <-  numeric(9)\n# starting guess for search is eta = 2 * sigma * max, mu = 15, sigma=4\n# max for each year is from observed counts.\npar[1:4] <- c(2*4.3*11000, 2*4.3*4100, 2*4.3*700, 2*4.3*4100)\n#  result: 94600 35260  6020 35260\n#  start for yearly timing (mu) is day of observed maximum\npar[5:8] <- c(15,15,19,15)\n# start for spread, sigma, is from preceding fit.\npar[9] <- 4.3\n# the required precision of fit, reltol, is reduced from the default.\nGfit <- optim(par, GssqAll, dat=dat[,1:3], control=list(reltol=0.01) )\npar = Gfit$par; # fitted parameters\npr = round(par, 2) # rounded for printing.\na <- data.frame (Year=1996:1999,eta=pr[1:4],mu=pr[5:8], sigma=pr[9])\nkbl(a)\n# r^2 for overall fit\nSSQ <- sum( (dat[,3] - mean(xa[,2]))^2 ) # SSQ from mean (dumb)\nr2  <-  1- Gfit$value / SSQ\n# print(paste(' r^2 =', round (r2,2) ))\n```\n\nThe resulting fit, with $r^2 =$ `r round (r2,2)`, minimizes SSQ over all years, despite large differences in abundance between years. Perhaps large abundances have a larger effect on the fit than small. To investigate, we determine the fit for each year as separate $r^2$ values.\n\n```{r}\neta <- par[1:4];  mu <- par[5:8]; sigma <- par[9]; \nyear <- unique(dat[,1]) # 1996 to 1999\nnyear <- length(year)   #4 \nyhat <- NULL # will be 21 predicted \n    r2 <- numeric(nyear)\n    for (j in 1:nyear) {\n        k <- which(dat[,1] == year[j]) # index for rows for this year\n        predicted <- eta[j] * dnorm(dat[k,2], mu[j], sigma) # a vector\n        # dat[,2] is day. dat[,3] is count\n        ssq_fit <- sum( (dat[k,3] - predicted      )^2)\n        ssq_raw <- sum( (dat[k,3] - mean(dat[k,3]) )^2)\n        r2[j] <- 1 - ssq_fit / ssq_raw\n        yhat <- c(yhat, predicted) # accumulate and save predicted\n    }\na <- data.frame(Year=year, Abundance =eta, r2=round(r2,2) )\nkbl(a) \n```\n\nCompare the estimates of \"fish days\" from trapezoidal AUC and from fitting Gaussian distributions.\n\n```{r}\na <- data.frame(Year=year, Abundance = round(eta,0),\n     AUC = c(101211, 56010, 9847,45263))\na[,\"Difference(%)\"] <- round( 100*(a$AUC-a$Abundance)/a$Abundance, 2)\nkbl(a)\na1 <- round(mean(abs(a$Difference)),0)\n#print( paste('The mean difference is ', a1, '%.',sep='' )) \n```\n\nThe mean difference in annual estimates of *fish days* is `r a1`%.'\n\nThe four Gaussian distributions, and the observed and predicted counts, are plotted in @fig-years by the following chunk.\n\n```{r}\n#| label: fig-years\n#| fig-cap: \"Four Gaussian curves with the same spread describe the observed salmon abundance. 5 or 6 observations in each year estimate just 2 parameters: abundance and timing.\"\n\n# GssqAllPred <- function(par, dat){\n#     # same as GssqAll but returns the predictions. AND smooth curves.\n#     eta <- par[1:4];   mu <- par[5:8]; sigma <- par[9];\n#     years <-  unique(dat[,1]) # 1996 to 1999\n#     nyears <- length(years)\n#     x <- 1:30 # days for smooth\n#     ybar <- numeric(dim(dat)[1] ) # predicted for each day\n#     smooth <- NULL # predicted as smooth curve\n#     m = 0 # row of output, total 21\n#     for (j in 1:nyears) {  # 1 to 4, 1996 to 1999\n#         k <- which(dat[,1] == years[j]) # find rows in a for each year\n#         for (i in k){ # each row in that year\n#             m <- m+1 # row of output\n#             ybar[m]  <- eta[j] * dnorm(dat[m,2],mu[j],sigma)\n#         }\n#         smooth <- c(smooth, eta[j] * dnorm(x,mu[j],sigma)) \n#     }\n#     a <- list(ybar=ybar, smooth=smooth)\n#     return(a)\n# }\n\nx <- c(1:30) # days of September\n# yhat is predicted counts for the days observed\nymax= 1.1e-3 * max(yhat)  # plot as thousands\nSetPar()\nplot(1e-3*yhat ~ dat[,2], xlim=c(1,30), ylim=c(0, ymax), yaxs=\"i\",\n     xlab='Day', ylab=\"Salmon Counts ('000)\" )\npoints(1e-3*dat[,3] ~ dat[,2], pch=1) # observed\nfor (j in 1:4){\n    y  <- eta[j] * dnorm(x, mu[j], sigma) # smooth curve\n    lines(1e-3*y ~ x)\n}\nAxis34()\n```\n\n# Discussion\n\nThe overall result of fitting Gaussians to four years of data simultaneously produced $r^2=99\\%$ which seems excellent, but only superficially. At least we have\n\n1.  estimates of how well the data are fitted by the model;\n\n2.  a better way to interpolate between observations, the continuous Gaussian; and\n\n3.  removed the need for, and effect from, arbitrary estimates for timing of zero abundance.\n\nBut there is a tendency for\n\n1.  larger abundances to have better fit ($r^2$ by year), suggesting that large observations have a bigger effect than small;\n\n2.  and years with the largest difference between Gaussian and AUC have lower fit.\n\nThe range of observations, from 15 to 11,000, suggests that the sampling error, the precision of observations, is proportional to the observation, as in $\\pm 20 \\%,$ as opposed to a fixed error, as in $\\sigma=20$ *fish.* Ordinary linear regression assumes fixed error, and the non-linear fitting mimics that. Proportional sampling error, so the error is essentially a multiplier, implies the fit should be to log abundances, so that the log (error) is additive.\n\n## Next Step\n\nSaved for a subsequent post: Determine how to fit the log of the Gaussian to the log of the counts. I discovered a mathematical **trick** that makes this simple.\n\n## Down the Road\n\nThese regressions should be more rigorous, as Bayesian multi-level regression. And what about multiple rivers in the same area that are surveyed in the same years -- can the model can be extended to river effects similar to year effects?\n\n# References\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.36","editor":"visual","theme":"simplex","title-block-banner":true,"title":"Spawner Surveys: Not Year By Year.","author":"Scott Akenhead","date":"2022-08-22","categories":["analysis","spawner","survey","multi-level"],"bibliography":["references.bib"]},"extensions":{"book":{"multiFile":true}}}}}