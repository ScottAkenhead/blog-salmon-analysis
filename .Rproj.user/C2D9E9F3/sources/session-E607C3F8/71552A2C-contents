---
title: "Spawner Surveys: Not Year By Year."
author: "Scott Akenhead"
date: "2022-08-22"
categories: [analysis, spawner, survey, multi-level]
bibliography: references.bib
---

My first post via [Quarto blog](https://quarto.org/docs/websites/website-blog.html "link:"), please forebear.

![](thumbnail.jpg)

Since this post doesn't specify an explicit `image`, the first image in the post will be used in the listing page of posts.

```{r, setup}
#| include: false
#| echo: false
library(knitr); library(kableExtra);library(kableExtra);
knitr::opts_chunk$set(
  comment = '', fig.width = 6, fig.height = 6, collapse = TRUE, hold = TRUE)
```

```{r, local}
#| include: false
#| echo: false
# ________ local functions _________
Axis34 <- function(){
	axis(side=3,labels=FALSE);axis(side=4,labels=FALSE);box(lwd=1.5)
} 
SetPar <-function(){
    par(tcl=0.2,          # axis tics small inside vs -0.5: large outside
   	las=1,                # axis numbers always horizontal
   	cex.axis=1.2,         # larger axis numbers 
	cex.lab=1.5,          # big axis labels
	mgp=c(1.75, 0.2, 0),  # closer axis labels and numbers
   	pch=20	              # small dots
)}
```

# Introduction

This blog describes a simplistic examination of surveys that estimate the number of salmon spawners in a river. Several surveys are conducted each year (methods irrelevant here). Two goals

1.  Convince biologists that analyzing all of the data at once is better than one year at a time. So we will be looking for an overall pattern of salmon, a shape, and extracting a *years effect* on that pattern -- which will include yearly abundance of salmon, the goal of the surveys.

2.  Compare the results of conventional year-by-year approach to the result from years-together.

The analysis is deliberately simple, with more sophisticated approaches planned for subsequent blogs.

## Area Under the Curve (AUC)

Convention determines salmon spawner abundance by linear interpolation between survey estimates each year separately, with assumptions about timing for zero abundance before and after the surveys: a **trapezoid** [@parken2003]. There is an alternative, where all years are combined to

(a) determine the general temporal pattern of spawner abundance ("*salmon days*), perhaps Gaussian if early arrivals and late survivors viewed as simply rare, perhaps Beta if ordained zero.
(b) given (a), estimate the change to that shape for each year, perhaps skewed to be earlier or later than the pattern across all years,
(c) given (a) and (b), determine the total abundance for each year, and
(d) from (a), (b), and (c), determine the observation error for individual surveys, $\sigma_{obs},$ perhaps as a fixed percentage error ($\sigma_{obs}=20\%$ of true abundance, implying a log transform) rather than an absolute error ($\sigma_{obs} = 20$ spawners).

# Example

## Test Dataset

As test data, the abundance estimates for 21 surveys over 4 years from Parken *et al.* 2003 (@fig-Parken). The numbers are reproduced only approximately (@tbl-raw), and plotted in @fig-raw with the observations from all years combined.

![Plot of trapezoids from salmon spawner surveys from Parken et al (2003).](AUC%20Fig%202%20Parken%20Bailey%20Irvine%202003.png){#fig-Parken}

```{r}
#| label: tbl-raw
#| tbl-cap: "Salmon counts as read from Parken _et al._ (2003)."

dat <- data.frame(
    year=c(rep(1996,5),rep(1997,5),rep(1998,6),rep(1999,5)), 
    day=c(5,9,13,18,23,  5,9,13,15,19,  5,9,13,15,19,23,  5,9,13,15,19),
    count=c(850,6000,11000,5000,1000,  500,2700,4000,4100,2500,
            15,40,200,500,700,600,  500,2500,4100,4000,1300) )
kbl(dat)
```

```{r}
#| label: fig-raw
#| fig-cap: "Salmon counts as read from Parken _et al._ (2003) by day with all years combined."

SetPar()
plot(dat$day, 0.001*dat$count, xlim=c(0,36),ylim=c(0,12),yaxs="i",
     xlab='Day from 31 October', ylab="Salmon Count ('000)")
Axis34()
```

### Scale by Maximum Count

First guess at shape is to scale observations by maximum each year, then fit a loess curve (@fig-scaled). It is necessary to sort the data by day for loess. Zeros were added outside the dates of observations, as per using a trapezoid.

```{r}
#| label: fig-scaled
#| fig-cap: "Salmon counts scaled (from 0 to 1) by the maximum count each year." 

xa <- by(dat$count,dat$year, max) # get maximums by year
xb <- by(dat$count,dat$year, length) # number of observations by year
# one value per year, copy for each obs in a year.
xc <- NULL  # temporary
for(j in 1:4) xc <- c(xc,rep( xa[[j]], xb[[j]]) )    
dat$max <- xc 
dat$scaled <- dat$count / dat$max
xa <- dat[order(dat$day), c("day","scaled")] # extract and sort
lo <- loess(scaled ~ day, data=xa) # fit smoother to sorted data
#  range for resulting smooth line is range of observed days
days = 5:23
loPred <- predict(lo, data.frame(day = days) ) # all days in range
loPred <-data.frame(pred=loPred,day=days)      # not just observed day

SetPar()
plot(scaled~day,data=xa, xlim=c(0,36),
     xlab='Day from 31 October', ylab='Scaled Count')
lines(loPred$pred  ~ loPred$day ) 
Axis34()
```

## Shift Observations by Day of Maximum

One of the years has a large change in timing, adding scatter to the preceding plot. We can reduce that scatter shifting (aligning) observations according to date of maximum abundance within years (@fig-shifted). This is inaccurate (a hueristic) but demonstrates a parameter (timing) that will be estimated better subsequently. The variable *shift* is the shifted value for day of observation, with negative values and the peak will be at day *shift*. Apply the shift and plot.

```{r}
#| label: fig-shifted
#| fig-cap: "Salmon counts shifted and scaled (from 0 to 1) by the maximum count, and the corresponding day, within each year." 

peak <-dat$day[ which(dat$count == dat$max)] # peak day by year 
xa <- NULL # temporary
for(j in 1:4) xa <- c(xa,rep( peak[j], xb[[j]]) )    
dat$shift <- dat$day - xa 
xa <- dat[order(dat$shift), c("shift","scaled")] # extract and sort
# add zeros outside range of obs.
xa0 <- rbind (c(-15,0), xa, c(15,0))
lo <- loess(scaled ~ shift, data=xa0) # smoother #,span=0.5)
loPred <- predict(lo, data.frame(shift= -15:15) )
loPred <-data.frame(pred=loPred,shift=c(-15:15))
# redo preceding plot
SetPar()
plot(scaled~shift,data=xa0, xlim=c(-18,18), ylim=c(0,1), yaxs="i",
     xlab='Day from Year Peak', ylab='Scaled Count')
lines(pred ~shift, data = loPred ) # 
Axis34()
```

## Assume Gaussian

@fig-shifted suggests a symmetric Gaussian ("normal") probability density distribution (PDD):

$$ \textbf{G}(x) = (2\pi \sigma)^{-1/2} ~ e^{ -(x-\mu)^2 /2\sigma ^2} $$

with $\mu \approx 0 \text{ and } \sigma \approx 5$ days as noticed from crude scaling of abundance and shifting of time for each year.

As well as the parameters $\mu$  and $\sigma,$ additional parameter $\eta$ is required for total abundance, the integral of the PDD and proportional to maximum abundance.

The integral of the Gauss-Laplace function $$\int{e^{-x^2}} = (2\pi)^{1/2} \approx 2.5$$ For a PDD, the integral must be 1, and this is effected by the term $(2\pi \sigma )^{-1/2} \approx 0.4.$ This is the maximum of the Gaussian PDD at $\mu = 0$ when $\sigma = 1.$ If the abundance $\eta= 1/0.3989422804 = 2.505528$ then the maximum for the curve is 1 and the integral $\approx 2.5.$ Similarly, when $\sigma = 2$ the maximum is 1 when $\eta= 2/0.4 = 5$ and for $\sigma = 5, ~ \eta=12.5$

In the scaled and shifted data the maximum was assigned to be 1 and it was observed that $\sigma \approx 5.$ Fitting a Gaussian distribution with 3 parameters $(\eta,\mu,\sigma)$ to all years combined, and after the rough scaling and shifting, is accomplished by the following chunk.

This chunk is a non-linear fit, obtained by searching for the parameters that have the lowest SSQ, the lowest sum of squared differences between observed and predicted (fitted) values. A function *Gssq()* is defined for this; it uses the built-in function stats::dnorm (normal probability density) to predict $\hat{y}$ for each observed day, given trial estimates for $\mu,~ \sigma,~ \eta$ and determines the SSQ. Then *Gssq()* is a parameter for the built-in function stats::optim that searches for value of the parameters that minimize SSQ. The other parameters for *optim()* are (1) a starting guess at parameters, and (2) the observed data: 21 values of *day* and *abundance*.

Fitting is by searching for the set of parameters that minimizes the sum of squared differnces (SSQ) between predicted $\hat{y}$ and observed $y$, $$ SSQ = \sum{(y - \hat{y})^2}$$

Given the SSQ at best fit, we can determine how well the data is fitted by a Gaussian, $\hat{y},$ compared to fitting the mean abundance over all days, $\bar{y},$ a default model, by determining the reduction in SSQ: $$ r^2 = 1- \frac{\sum (y-\hat{y})^2}{\sum (y-\bar{y})^2 }.$$

```{r}
Gssq <- function(par, xy){
    x <- xy[,1]; y<-  xy[,2];
    eta <- par[1]; mu <- par[2]; sigma <- par[3];
    ybar <- eta * dnorm(x,mu,sigma)
    ssq <- sum( (y-ybar)^2)
    return(ssq)
}
Gfit <- optim(par=c(12.5, 0, 5), Gssq, xy=xa )
par = Gfit$par; pr = round(par,2)
SSQ <- sum( (xa[,2] - mean(xa[,2]))^2 ) # sum of y minus y-hat, squared
r2 <- round (1- Gfit$value/SSQ, 2) # Gfit$value is SSQ fitted. 
cat('r^2 = ', r2, ', eta = ',pr[1], ', mu = ',pr[2],
    ', sigma = ', pr[3], '\n', sep='')
```

The result provides $\sigma \approx 4.3$ which is a more narrow spread than noticing the inflection point in the Gaussian, at $(1 ~\sigma),$ was about $\pm5$ from the mean. The fitted maximum will necessarily be close to 1 because of our scaling, but in this case greater than 1, given $\sigma / 0.4 = 10.72$ but $\eta = 11.2,$ due to our rough scaling.

The fitted line is compared visually to the data by the following chunk. A smooth Gaussian curve is plotted by estimating a value for 18 days before and after the mean day.

```{r}
#| label: fig-norm
#| fig-cap: "Gaussian probability density fitted to the scaled and shifted counts."

x <- c(-18:18) # 37 points for a smooth(er) line.
pred <- par[1] * dnorm(x, par[2],par[3]) # eta, mu, sigma
SetPar()
plot(scaled~shift,data=xa, xlim=c(-18,18), ylim=c(0,1.2),yaxs="i",
     xlab='Day from Annual Peak', ylab='Scaled Count')
lines(pred ~ x)
Axis34()
```

## Year Effects

The preceding identified the overall pattern as Gaussian with $\sigma \approx 4.3.$ If $\sigma$ is assumed to be constant across years, as @fig-norm suggests, then only two observations per year are required to estimate total abundance: $\eta_t,~ \mu_t.$ To clarify: a single observation of early and small abundance cannot distinguish a small run with average timing from a big run that is late; one subsequent observation can (in theory).

This test dataset has 5 or 6 observations per year, so precision is improved and over-fitting is reduced compared to (a) fitting 3 parameters to each year separately, or (b) interpolating a trapezoid that includes arbitrarily placed zeros.

The next step estimates 8 parameters for year effects: $\eta$ (abundance) and $\mu$ (timing) for each of 4 years, and 1 estimate for $\sigma$ (spread) from 21 observations. The ratio of $2.\bar{3}$ data points to parameters might be improved by adding more years: 20 years \@ 5 surveys/year would be $11.\bar{1}.$

$$\hat{y}_{day, year} = \eta_{year} ~ \textbf{G}(x_{year},\mu_{year},\sigma)$$

```{r}
#| warning: false

GssqAll <- function(par, dat){
    # dat columns: year, day, count
    # par is 9 parameters
    eta <- par[1:4]; # abundance each year, length 4
    mu <- par[5:8]; # timing  each year, length 4
    sigma <- par[9]; # spread every yearr, length 1
    years = unique(dat[,1]) # 1996 to 1999
    ybar <- numeric(length(dat[,1])) # the predicted, length 21
    m = 0  # starting index for output vector ybar
    for (j in 1:length(years)) {  # 4 years: 1996 to 1999
        k <- which(dat[,1] == years[j]) # find rows in dat for each year
        for (i in k){ # each observed day in that year
            m <- m+1 # advance index for output
            ybar[m]  <- eta[j] * dnorm(dat[m,2],mu[j],sigma)
            # predicted: abundance times normal(day, timing, spread)
        }
    }
    ssq <- sum( (dat[,3]-ybar)^2) # result of trial values for par
    return(ssq)
}
# par is total abundance 4 years:eta; timing 4 years: mu, and 
# spread for all years: sigma. total 9 parameters.
par <-  numeric(9)
# starting guess for search is eta = 2 * sigma * max, mu = 15, sigma=4
# max for each year is from observed counts.
par[1:4] <- c(2*4.3*11000, 2*4.3*4100, 2*4.3*700, 2*4.3*4100)
#  result: 94600 35260  6020 35260
#  start for yearly timing (mu) is day of observed maximum
par[5:8] <- c(15,15,19,15)
# start for spread, sigma, is from preceding fit.
par[9] <- 4.3
# the required precision of fit, reltol, is reduced from the default.
Gfit <- optim(par, GssqAll, dat=dat[,1:3], control=list(reltol=0.01) )
par = Gfit$par; # fitted parameters
pr = round(par, 2) # rounded for printing.
a <- data.frame (Year=1996:1999,eta=pr[1:4],mu=pr[5:8], sigma=pr[9])
kbl(a)
# r^2 for overall fit
SSQ <- sum( (dat[,3] - mean(xa[,2]))^2 ) # SSQ from mean (dumb)
r2  <-  1- Gfit$value / SSQ
# print(paste(' r^2 =', round (r2,2) ))
```

The resulting fit, with $r^2 =$ `r round (r2,2)`, minimizes SSQ over all years, despite large differences in abundance between years. Perhaps large abundances have a larger effect on the fit than small. To investigate, we determine the fit for each year as separate $r^2$ values.

```{r}
eta <- par[1:4];  mu <- par[5:8]; sigma <- par[9]; 
year <- unique(dat[,1]) # 1996 to 1999
nyear <- length(year)   #4 
yhat <- NULL # will be 21 predicted 
    r2 <- numeric(nyear)
    for (j in 1:nyear) {
        k <- which(dat[,1] == year[j]) # index for rows for this year
        predicted <- eta[j] * dnorm(dat[k,2], mu[j], sigma) # a vector
        # dat[,2] is day. dat[,3] is count
        ssq_fit <- sum( (dat[k,3] - predicted      )^2)
        ssq_raw <- sum( (dat[k,3] - mean(dat[k,3]) )^2)
        r2[j] <- 1 - ssq_fit / ssq_raw
        yhat <- c(yhat, predicted) # accumulate and save predicted
    }
a <- data.frame(Year=year, Abundance =eta, r2=round(r2,2) )
kbl(a) 
```

Compare the estimates of "fish days" from trapezoidal AUC and from fitting Gaussian distributions.

```{r}
a <- data.frame(Year=year, Abundance = round(eta,0),
     AUC = c(101211, 56010, 9847,45263))
a[,"Difference(%)"] <- round( 100*(a$AUC-a$Abundance)/a$Abundance, 2)
kbl(a)
a1 <- round(mean(abs(a$Difference)),0)
#print( paste('The mean difference is ', a1, '%.',sep='' )) 
```

The mean difference in annual estimates of *fish days* is `r a1`%.'

The four Gaussian distributions, and the observed and predicted counts, are plotted in @fig-years by the following chunk.

```{r}
#| label: fig-years
#| fig-cap: "Four Gaussian curves with the same spread describe the observed salmon abundance. 5 or 6 observations in each year estimate just 2 parameters: abundance and timing."

# GssqAllPred <- function(par, dat){
#     # same as GssqAll but returns the predictions. AND smooth curves.
#     eta <- par[1:4];   mu <- par[5:8]; sigma <- par[9];
#     years <-  unique(dat[,1]) # 1996 to 1999
#     nyears <- length(years)
#     x <- 1:30 # days for smooth
#     ybar <- numeric(dim(dat)[1] ) # predicted for each day
#     smooth <- NULL # predicted as smooth curve
#     m = 0 # row of output, total 21
#     for (j in 1:nyears) {  # 1 to 4, 1996 to 1999
#         k <- which(dat[,1] == years[j]) # find rows in a for each year
#         for (i in k){ # each row in that year
#             m <- m+1 # row of output
#             ybar[m]  <- eta[j] * dnorm(dat[m,2],mu[j],sigma)
#         }
#         smooth <- c(smooth, eta[j] * dnorm(x,mu[j],sigma)) 
#     }
#     a <- list(ybar=ybar, smooth=smooth)
#     return(a)
# }

x <- c(1:30) # days of September
# yhat is predicted counts for the days observed
ymax= 1.1e-3 * max(yhat)  # plot as thousands
SetPar()
plot(1e-3*yhat ~ dat[,2], xlim=c(1,30), ylim=c(0, ymax), yaxs="i",
     xlab='Day', ylab="Salmon Counts ('000)" )
points(1e-3*dat[,3] ~ dat[,2], pch=1) # observed
for (j in 1:4){
    y  <- eta[j] * dnorm(x, mu[j], sigma) # smooth curve
    lines(1e-3*y ~ x)
}
Axis34()
```

# Discussion

The overall result of fitting Gaussians to four years of data simultaneously produced $r^2=99\%$ which seems excellent, but only superficially. At least we have

1.  estimates of how well the data are fitted by the model;

2.  a better way to interpolate between observations, the continuous Gaussian; and

3.  removed the need for, and effect from, arbitrary estimates for timing of zero abundance.

But there is a tendency for

1.  larger abundances to have better fit ($r^2$ by year), suggesting that large observations have a bigger effect than small;

2.  and years with the largest difference between Gaussian and AUC have lower fit.

The range of observations, from 15 to 11,000, suggests that the sampling error, the precision of observations, is proportional to the observation, as in $\pm 20 \%,$ as opposed to a fixed error, as in $\sigma=20$ *fish.* Ordinary linear regression assumes fixed error, and the non-linear fitting mimics that. Proportional sampling error, so the error is essentially a multiplier, implies the fit should be to log abundances, so that the log (error) is additive.

## Next Step

Saved for a subsequent post: Determine how to fit the log of the Gaussian to the log of the counts. I discovered a mathematical **trick** that makes this simple.

## Down the Road

These regressions should be more rigorous, as Bayesian multi-level regression. And what about multiple rivers in the same area that are surveyed in the same years -- can the model can be extended to river effects similar to year effects?

# References
