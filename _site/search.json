[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Context! What’s Happening?",
    "section": "",
    "text": "This is me: ORCID:0000-0003-1218-3118. The last decade was a return to my roots as a marine fisheries ecologist, working with scientists at the Pacific Biological Station. This blog revisits and updates some of those analyses, to provide examples that might be helpful to ecological detectives trying to solve salmon mysteries (while learning statistics in R).\nSalmon are declining almost everywhere, exceptions mainly northern populations in Alaska and Labrador. After >30 years of these declines, we know this is because marine survival (the survival of smolts leaving freshwater until returning from the sea as adults) has declined from ~10% to ~1%, but we do not know why. Marine survival is failing in pristine watersheds as well as industrialized watersheds.\nWhat are humans doing wrong to have such a massive and widespread effect? It is not over-fishing. Maybe pollution, or global warming, or hatcheries?\nRecently I have involved myself with IYS and LSF and most recently a project relate to these, Salmon Data Mobilization (paper in prep) as means to new salmon analyses and models.\nMaybe we should be throwing this problem, this mystery, out to a wider community of ecologists, analysts, and coders. How, though? Seems unlikely that this blog will help, but let’s see.\n\n\n\nFigure. Spawning Sockeye Salmon spawning in just the right gravel after migrating thousands of kilometer to distant seas and back to the river they were born in."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Spawner Surveys: Not Year By Year.",
    "section": "",
    "text": "A simplistic examination of surveys that estimate the number of salmon spawners in a river."
  },
  {
    "objectID": "posts/welcome/index.html#area-under-the-curve-auc",
    "href": "posts/welcome/index.html#area-under-the-curve-auc",
    "title": "Spawner Surveys: Not Year By Year.",
    "section": "Area Under the Curve (AUC)",
    "text": "Area Under the Curve (AUC)\nConvention determines salmon spawner abundance by linear interpolation between survey estimates each year separately, with assumptions about timing for zero abundance before and after the surveys: a trapezoid (Parken, Bailey, and Irvine 2003). There is an alternative, where all years are combined to\n\ndetermine the general temporal pattern of spawner abundance (“salmon days), perhaps Gaussian if early arrivals and late survivors viewed as simply rare, perhaps Beta if ordained zero.\ngiven (a), estimate the change to that shape for each year, perhaps skewed to be earlier or later than the pattern across all years,\ngiven (a) and (b), determine the total abundance for each year, and\nfrom (a), (b), and (c), determine the observation error for individual surveys, \\(\\sigma_{obs},\\) perhaps as a fixed percentage error (\\(\\sigma_{obs}=20\\%\\) of true abundance, implying a log transform) rather than an absolute error (\\(\\sigma_{obs} = 20\\) spawners)."
  },
  {
    "objectID": "posts/welcome/index.html#test-dataset",
    "href": "posts/welcome/index.html#test-dataset",
    "title": "Spawner Surveys: Not Year By Year.",
    "section": "Test Dataset",
    "text": "Test Dataset\nAs test data, the abundance estimates for 21 surveys over 4 years from Parken et al. 2003 (Figure 1). The numbers are reproduced only approximately (Table 1), and plotted in Figure 2 with the observations from all years combined.\n\n\n\nFigure 1: Plot of trapezoids from salmon spawner surveys from Parken et al (2003).\n\n\n\ndat <- data.frame(\n    year=c(rep(1996,5),rep(1997,5),rep(1998,6),rep(1999,5)), \n    day=c(5,9,13,18,23,  5,9,13,15,19,  5,9,13,15,19,23,  5,9,13,15,19),\n    count=c(850,6000,11000,5000,1000,  500,2700,4000,4100,2500,\n            15,40,200,500,700,600,  500,2500,4100,4000,1300) )\nkbl(dat)\n\n\n\nTable 1:  Salmon counts as read from Parken et al. (2003). \n \n  \n    year \n    day \n    count \n  \n \n\n  \n    1996 \n    5 \n    850 \n  \n  \n    1996 \n    9 \n    6000 \n  \n  \n    1996 \n    13 \n    11000 \n  \n  \n    1996 \n    18 \n    5000 \n  \n  \n    1996 \n    23 \n    1000 \n  \n  \n    1997 \n    5 \n    500 \n  \n  \n    1997 \n    9 \n    2700 \n  \n  \n    1997 \n    13 \n    4000 \n  \n  \n    1997 \n    15 \n    4100 \n  \n  \n    1997 \n    19 \n    2500 \n  \n  \n    1998 \n    5 \n    15 \n  \n  \n    1998 \n    9 \n    40 \n  \n  \n    1998 \n    13 \n    200 \n  \n  \n    1998 \n    15 \n    500 \n  \n  \n    1998 \n    19 \n    700 \n  \n  \n    1998 \n    23 \n    600 \n  \n  \n    1999 \n    5 \n    500 \n  \n  \n    1999 \n    9 \n    2500 \n  \n  \n    1999 \n    13 \n    4100 \n  \n  \n    1999 \n    15 \n    4000 \n  \n  \n    1999 \n    19 \n    1300 \n  \n\n\n\n\n\n\n\nSetPar()\nplot(dat$day, 0.001*dat$count, xlim=c(0,36),ylim=c(0,12),yaxs=\"i\",\n     xlab='Day from 31 October', ylab=\"Salmon Count ('000)\")\nAxis34()\n\n\n\n\nFigure 2: Salmon counts as read from Parken et al. (2003) by day with all years combined.\n\n\n\n\n\nScale by Maximum Count\nFirst guess at shape is to scale observations by maximum each year, then fit a loess curve (Figure 3). It is necessary to sort the data by day for loess. Zeros were added outside the dates of observations, as per using a trapezoid.\n\nxa <- by(dat$count,dat$year, max) # get maximums by year\nxb <- by(dat$count,dat$year, length) # number of observations by year\n# one value per year, copy for each obs in a year.\nxc <- NULL  # temporary\nfor(j in 1:4) xc <- c(xc,rep( xa[[j]], xb[[j]]) )    \ndat$max <- xc \ndat$scaled <- dat$count / dat$max\nxa <- dat[order(dat$day), c(\"day\",\"scaled\")] # extract and sort\nlo <- loess(scaled ~ day, data=xa) # fit smoother to sorted data\n#  range for resulting smooth line is range of observed days\ndays = 5:23\nloPred <- predict(lo, data.frame(day = days) ) # all days in range\nloPred <-data.frame(pred=loPred,day=days)      # not just observed day\n\nSetPar()\nplot(scaled~day,data=xa, xlim=c(0,36),\n     xlab='Day from 31 October', ylab='Scaled Count')\nlines(loPred$pred  ~ loPred$day ) \nAxis34()\n\n\n\n\nFigure 3: Salmon counts scaled (from 0 to 1) by the maximum count each year."
  },
  {
    "objectID": "posts/welcome/index.html#shift-observations-by-day-of-maximum",
    "href": "posts/welcome/index.html#shift-observations-by-day-of-maximum",
    "title": "Spawner Surveys: Not Year By Year.",
    "section": "Shift Observations by Day of Maximum",
    "text": "Shift Observations by Day of Maximum\nOne of the years has a large change in timing, adding scatter to the preceding plot. We can reduce that scatter shifting (aligning) observations according to date of maximum abundance within years (Figure 4). This is inaccurate (a hueristic) but demonstrates a parameter (timing) that will be estimated better subsequently. The variable shift is the shifted value for day of observation, with negative values and the peak will be at day shift. Apply the shift and plot.\n\npeak <-dat$day[ which(dat$count == dat$max)] # peak day by year \nxa <- NULL # temporary\nfor(j in 1:4) xa <- c(xa,rep( peak[j], xb[[j]]) )    \ndat$shift <- dat$day - xa \nxa <- dat[order(dat$shift), c(\"shift\",\"scaled\")] # extract and sort\n# add zeros outside range of obs.\nxa0 <- rbind (c(-15,0), xa, c(15,0))\nlo <- loess(scaled ~ shift, data=xa0) # smoother #,span=0.5)\nloPred <- predict(lo, data.frame(shift= -15:15) )\nloPred <-data.frame(pred=loPred,shift=c(-15:15))\n# redo preceding plot\nSetPar()\nplot(scaled~shift,data=xa0, xlim=c(-18,18), ylim=c(0,1), yaxs=\"i\",\n     xlab='Day from Year Peak', ylab='Scaled Count')\nlines(pred ~shift, data = loPred ) # \nAxis34()\n\n\n\n\nFigure 4: Salmon counts shifted and scaled (from 0 to 1) by the maximum count, and the corresponding day, within each year."
  },
  {
    "objectID": "posts/welcome/index.html#assume-gaussian",
    "href": "posts/welcome/index.html#assume-gaussian",
    "title": "Spawner Surveys: Not Year By Year.",
    "section": "Assume Gaussian",
    "text": "Assume Gaussian\nFigure 4 suggests a symmetric Gaussian (“normal”) probability density distribution (PDD):\n\\[ \\textbf{G}(x) = (2\\pi \\sigma)^{-1/2} ~ e^{ -(x-\\mu)^2 /2\\sigma ^2} \\]\nwith \\(\\mu \\approx 0 \\text{ and } \\sigma \\approx 5\\) days as noticed from crude scaling of abundance and shifting of time for each year.\nAs well as the parameters \\(\\mu\\)  and \\(\\sigma,\\) additional parameter \\(\\eta\\) is required for total abundance, the integral of the PDD and proportional to maximum abundance.\nThe integral of the Gauss-Laplace function \\[\\int{e^{-x^2}} = (2\\pi)^{1/2} \\approx 2.5\\] For a PDD, the integral must be 1, and this is effected by the term \\((2\\pi \\sigma )^{-1/2} \\approx 0.4.\\) This is the maximum of the Gaussian PDD at \\(\\mu = 0\\) when \\(\\sigma = 1.\\) If the abundance \\(\\eta= 1/0.3989422804 = 2.505528\\) then the maximum for the curve is 1 and the integral \\(\\approx 2.5.\\) Similarly, when \\(\\sigma = 2\\) the maximum is 1 when \\(\\eta= 2/0.4 = 5\\) and for \\(\\sigma = 5, ~ \\eta=12.5\\)\nIn the scaled and shifted data the maximum was assigned to be 1 and it can be observed that \\(\\sigma \\approx 5.\\) Fitting a Gaussian distribution with 3 parameters \\((\\eta,\\mu,\\sigma)\\) to all years combined, and after the rough scaling and shifting, is accomplished by the following chunk.\nThis chunk is a non-linear fit, obtained by searching for the parameters that have the lowest SSQ, the lowest sum of squared differences between observed and predicted (fitted) values. A function Gssq() is defined for this; it uses the built-in function stats::dnorm (normal probability density) to predict \\(\\hat{y}\\) for each observed day, given trial estimates for \\(\\mu,~ \\sigma,~ \\eta\\) and determines the SSQ. Then Gssq() is a parameter for the built-in function stats::optim that searches for value of the parameters that minimize SSQ. The other parameters for optim() are (1) a starting guess at parameters, and (2) the observed data: 21 values of day and abundance.\nFitting is by searching for the set of parameters that minimizes the sum of squared differnces (SSQ) between predicted \\(\\hat{y}\\) and observed \\(y\\), \\[ SSQ = \\sum{(y - \\hat{y})^2}\\]\nGiven the SSQ at best fit, we can determine how well the data is fitted by a Gaussian, \\(\\hat{y},\\) compared to fitting the mean abundance over all days, \\(\\bar{y},\\) a default model, by determining the reduction in SSQ: \\[ r^2 = 1- \\frac{\\sum (y-\\hat{y})^2}{\\sum (y-\\bar{y})^2 }.\\]\n\nGssq <- function(par, xy){\n    x <- xy[,1]; y <-  xy[,2];\n    eta <- par[1]; mu <- par[2]; sigma <- par[3];\n    yhat <- eta * dnorm(x,mu,sigma) # make the prediction\n    ssq <- sum( (y-yhat)^2)   # to be minimized\n    return(ssq)\n}\n# starting guess for the search: eta=12.5, mu=0, sigma=5\nGfit <- optim(par=c(12.5, 0, 5), Gssq, xy=xa )\n# pull out the fitted parameters\npar = Gfit$par; pr = round(par,2)\n# Gfit$value is SSQ: fitted to observed. Get SSQ: mean to obs.\nSSQ <- sum( (xa[,2] - mean(xa[,2]))^2 ) #  y minus mean(y)\nr2 <- round (1- Gfit$value/SSQ, 2) # percent reduction in SSQ\ncat('r^2 = ', r2, ', eta = ',pr[1], ', mu = ',pr[2],\n    ', sigma = ', pr[3], '\\n', sep='')\n->  r^2 = 0.94, eta = 11.02, mu = 0.05, sigma = 4.29\n\nThe result provides \\(\\sigma \\approx 4.3\\) which is a more narrow spread than noticing the inflection point in the Gaussian, at \\((1 ~\\sigma),\\) was about \\(\\pm5\\) from the mean. The fitted maximum will necessarily be close to 1 because of our scaling, but in this case greater than 1, given \\(\\sigma / 0.4 = 10.72\\) but \\(\\eta = 11.2,\\) due to our rough scaling.\nThe fitted line is compared visually to the data by the following chunk. A smooth Gaussian curve is plotted by estimating a value for 18 days before and after the mean day.\n\nx <- c(-18:18) # 37 points for a smooth(er) line.\npred <- par[1] * dnorm(x, par[2],par[3]) # eta, mu, sigma\nSetPar()\nplot(scaled~shift,data=xa, xlim=c(-18,18), ylim=c(0,1.2),yaxs=\"i\",\n     xlab='Day from Annual Peak', ylab='Scaled Count')\nlines(pred ~ x)\nAxis34()\n\n\n\n\nFigure 5: Gaussian probability density fitted to the scaled and shifted counts."
  },
  {
    "objectID": "posts/welcome/index.html#year-effects",
    "href": "posts/welcome/index.html#year-effects",
    "title": "Spawner Surveys: Not Year By Year.",
    "section": "Year Effects",
    "text": "Year Effects\nThe preceding identified the overall pattern as Gaussian with \\(\\sigma \\approx 4.3.\\) If \\(\\sigma\\) is assumed to be known and constant across years, as Figure 5 suggests, then only two observations per year are required to estimate total abundance: \\(\\eta_t,~ \\mu_t.\\) To clarify: a single observation of early and small abundance cannot distinguish a small run with average timing from a big run that is late; one subsequent observation can make that distinction (in theory, if observations precise and salmon behave).\nThis test dataset has 5 or 6 observations per year, so precision is improved and over-fitting is reduced compared to (a) fitting 3 parameters to each year separately, or (b) interpolating to an aribitrary trapezoid that requires arbitrarily placed zeros.\nThe next step estimates 2 parameters for year effects: \\(\\eta_{year}\\) (abundance) and \\(\\mu_{year}\\) (timing) for each of 4 years, and 1 estimate for \\(\\sigma\\) (spread, constant) from 21 observations. This is a ratio of \\(2.\\bar{3}\\) data points to parameters, and could be improved by adding more years: for 20 years @ 5 surveys/year the ratio would be \\(11.\\bar{1}.\\)\n\\[\\hat{y}_{day, year} = \\eta_{year} ~ \\textbf{G}(x_{year},\\mu_{year},\\sigma)\\]\n\n# GssqAll <- function(par, dat){\n#     # dat columns: year, day, count\n#     # par is 9 parameters\n#     eta <- par[1:4]; # abundance each year, length 4\n#     mu <- par[5:8]; # timing  each year, length 4\n#     sigma <- par[9]; # spread every yearr, length 1\n#     years = unique(dat[,1]) # 1996 to 1999\n#     ybar <- numeric(length(dat[,1])) # the predicted, length 21\n#     m = 0  # starting index for output vector ybar\n#     for (j in 1:length(years)) {  # 4 years: 1996 to 1999\n#         k <- which(dat[,1] == years[j]) # find rows in dat for each year\n#         for (i in k){ # each observed day in that year\n#             m <- m+1 # advance index for output\n#             ybar[m]  <- eta[j] * dnorm(dat[m,2],mu[j],sigma)\n#             # predicted: abundance times normal(day, timing, spread)\n#         }\n#     }\n#     ssq <- sum( (dat[,3]-ybar)^2) # result of trial values for par\n#     return(ssq)\n# }\nGssqAll <- function(par, dat){\n    # dat columns: year, day, count\n    # par is 9 parameters\n    eta <- par[1:4]; # abundance each year, length 4\n    mu <- par[5:8]; # timing  each year, length 4\n    sigma <- par[9]; # spread every yearr, length 1\n    years = unique(dat[,1]) # 1996 to 1999\n    ybar <- numeric(length(dat[,1])) # the predicted, length 21\n    for (j in 1:length(years)) {  # 4 years: 1996 to 1999\n        k <- which(dat[,1] == years[j]) # find rows in dat for each year\n        ybar[k]  <- eta[j] * dnorm(dat[k,2],mu[j],sigma) # all of k\n        # predicted: abundance times normal(day, timing, spread)\n    }\n    ssq <- sum( (dat[,3]-ybar)^2) # result of trial values for par\n    return(ssq)\n}\n\n# par is eta(4),  mu (4), sigma (1)\npar <-  numeric(9)\n# starting guess for search is eta = 2*sigma*max.\n# where max is max observed count in each year.\npar[1:4] <- c(2*4.3*11000, 2*4.3*4100, 2*4.3*700, 2*4.3*4100)\n#  result: 94600 35260  6020 35260\n#  start for yearly timing (mu) is day of observed maximum each year\npar[5:8] <- c(15,15,19,15)\n# start for spread, sigma, is from preceding fit.\npar[9] <- 4.3\n# the required precision of fit, reltol, is reduced from the default.\ncontrol <- list(reltol=0.01)\nGfit <- optim(par, GssqAll, dat=dat[,1:3], control )\npar = Gfit$par; # 9 fitted parameters\npr[1:4] = round(par[1:4], 0) # rounded for printing.\npr[5:9] = round(par[5:9], 2) \na <- data.frame (Year=1996:1999,eta=pr[1:4],mu=pr[5:8], sigma=pr[9])\nkbl(a)\n\n\n\n \n  \n    Year \n    eta \n    mu \n    sigma \n  \n \n\n  \n    1996 \n    99334 \n    13.22 \n    3.72 \n  \n  \n    1997 \n    39584 \n    13.82 \n    3.72 \n  \n  \n    1998 \n    6024 \n    19.32 \n    3.72 \n  \n  \n    1999 \n    35662 \n    13.20 \n    3.72 \n  \n\n\n\n\n# r^2 for overall fit, all 4 years at once.\nSSQ <- sum( (dat[,3] - mean(xa[,2]))^2 ) # SSQ from mean\nr2  <-  1- Gfit$value / SSQ\nr2  <- 100*round (r2,2) # as percent\n# print( paste('r^2 = ', r2,'%', sep='') ) # save for text\n\nThe resulting fit, with \\(r^2 =\\) 99%, minimizes SSQ over all years, despite large differences in abundance within and between years. Perhaps large abundances have a larger effect than small on the fit determined by SSQ. To investigate, determine the fit for each year as separate \\(r^2\\) values.\n\neta <- par[1:4];  mu <- par[5:8]; sigma <- par[9]; \nyear <- unique(dat[,1]) # 1996 to 1999\nnyear <- length(year)   #4 \nyhat <- NULL # will be 21 predicted \nr2 <- numeric(nyear)\nfor (j in 1:nyear) {\n    k <- which(dat[,1] == year[j]) # index for rows for this year\n    yhat[k] <- eta[j] * dnorm(dat[k,2], mu[j], sigma) # a vector\n    # dat[,2] is day. dat[,3] is count\n    ssq_fit <- sum( (dat[k,3] - yhat[k])^2)\n    ssq_raw <- sum( (dat[k,3] - mean(dat[k,3]) )^2)\n    r2[j] <- 1 - ssq_fit / ssq_raw\n}\na <- data.frame(Year=year, Abundance=round(eta,0), r2=round(r2,2) ) \nkbl(a) \n\n\n\n \n  \n    Year \n    Abundance \n    r2 \n  \n \n\n  \n    1996 \n    99334 \n    0.99 \n  \n  \n    1997 \n    39584 \n    0.81 \n  \n  \n    1998 \n    6024 \n    0.82 \n  \n  \n    1999 \n    35662 \n    0.93 \n  \n\n\n\n\n\nCompare the estimates of “fish days” from trapezoidal AUC and from fitting Gaussian distributions.\n\na$AUC <- c(101211, 56010, 9847,45263)\na$PercentDiff <- round( 100*(a$AUC-a$Abundance)/a$Abundance, 2)\nkbl(a)\n\n\n\n \n  \n    Year \n    Abundance \n    r2 \n    AUC \n    PercentDiff \n  \n \n\n  \n    1996 \n    99334 \n    0.99 \n    101211 \n    1.89 \n  \n  \n    1997 \n    39584 \n    0.81 \n    56010 \n    41.50 \n  \n  \n    1998 \n    6024 \n    0.82 \n    9847 \n    63.46 \n  \n  \n    1999 \n    35662 \n    0.93 \n    45263 \n    26.92 \n  \n\n\n\n\na1 <- round(mean(abs(a$PercentDiff)),0)\n#print( paste('The mean difference is ', a1, '%.',sep='' )) \n\nThe mean difference in annual estimates of fish days is 33%.’\nThe four Gaussian distributions, and the observed and predicted counts, are plotted in Figure 6 by the following chunk.\n\n# GssqAllPred <- function(par, dat){\n#     # same as GssqAll but returns the predictions. AND smooth curves.\n#     eta <- par[1:4];   mu <- par[5:8]; sigma <- par[9];\n#     years <-  unique(dat[,1]) # 1996 to 1999\n#     nyears <- length(years)\n#     x <- 1:30 # days for smooth\n#     ybar <- numeric(dim(dat)[1] ) # predicted for each day\n#     smooth <- NULL # predicted as smooth curve\n#     m = 0 # row of output, total 21\n#     for (j in 1:nyears) {  # 1 to 4, 1996 to 1999\n#         k <- which(dat[,1] == years[j]) # find rows in a for each year\n#         for (i in k){ # each row in that year\n#             m <- m+1 # row of output\n#             ybar[m]  <- eta[j] * dnorm(dat[m,2],mu[j],sigma)\n#         }\n#         smooth <- c(smooth, eta[j] * dnorm(x,mu[j],sigma)) \n#     }\n#     a <- list(ybar=ybar, smooth=smooth)\n#     return(a)\n# }\n\nx <- c(1:30) # days of September\n# yhat is predicted counts for the days observed\nymax= 1.1e-3 * max(yhat)  # plot as thousands\nSetPar()\nplot(1e-3*yhat ~ dat[,2], xlim=c(1,30), ylim=c(0, ymax), yaxs=\"i\",\n     xlab='Day', ylab=\"Salmon Counts ('000)\" )\npoints(1e-3*dat[,3] ~ dat[,2], pch=1) # observed\nfor (j in 1:4){\n    y  <- eta[j] * dnorm(x, mu[j], sigma) # smooth curve\n    lines(1e-3*y ~ x)\n}\nAxis34()\n\n\n\n\nFigure 6: Four Gaussian curves with the same spread describe the observed salmon abundance. 5 or 6 observations in each year estimate just 2 parameters: abundance and timing."
  },
  {
    "objectID": "posts/welcome/index.html#next-step",
    "href": "posts/welcome/index.html#next-step",
    "title": "Spawner Surveys: Not Year By Year.",
    "section": "Next Step",
    "text": "Next Step\nSaved for a subsequent post: Determine how to fit the log of the Gaussian to the log of the counts. I discovered a mathematical trick that makes this simple."
  },
  {
    "objectID": "posts/welcome/index.html#down-the-road",
    "href": "posts/welcome/index.html#down-the-road",
    "title": "Spawner Surveys: Not Year By Year.",
    "section": "Down the Road",
    "text": "Down the Road\nThese regressions should be more rigorous, as Bayesian multi-level regression. And what about multiple rivers in the same area that are surveyed in the same years – can the model can be extended to river effects similar to year effects?"
  },
  {
    "objectID": "posts/new default post/UntitledQMD.html",
    "href": "posts/new default post/UntitledQMD.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod <- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds <- dat %>% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit > 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Helpful? Examples of Salmon Analysis with R",
    "section": "",
    "text": "news\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2022\n\n\nScott Akenhead\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nanalysis\n\n\nspawner\n\n\nsurvey\n\n\nmulti-level\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2022\n\n\nScott Akenhead\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n123\n\n\nSecond Tag\n\n\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is me: ORCID:0000-0003-1218-3118. Various adventures and tricks from analyzing salmon datasets seem worth sharing. The goal is to help ecological detectives analyze clues to solve the mystery: Who is (inadvertenly?) killing the salmon?"
  }
]